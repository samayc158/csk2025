Q1) Deep- neural feed forward (Boston Housing)

import tensorflow as tf
from tensorflow.keras.datasets import boston_housing
from sklearn import preprocessing
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
(train_x, train_y), (test_x, test_y) = boston_housing.load_data()
train_x = preprocessing.normalize(train_x)
test_x = preprocessing.normalize(test_x)
def HousePricePredictionModel():
  model = Sequential()
  model.add(Dense(64, activation='relu', input_shape=(train_x.shape[1],)))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(1))
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
  return model
model = HousePricePredictionModel()
model.fit(train_x, train_y, epochs=10, batch_size=1, verbose=1, validation_data=(test_x, test_y))



Q2) Feed forward Neural Netork (IMDB)

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense
# Load IMDB dataset (keep the top 10,000 most frequent words)
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)
# Pad sequences to ensure uniform input size (max length = 500 words)
X_train = pad_sequences(X_train, maxlen=500)
X_test = pad_sequences(X_test, maxlen=500)
# Define a feed-forward neural network
model = Sequential([
Embedding(input_dim=10000, output_dim=32, input_length=500), # Word embedding layer
    Flatten(), # Flatten to feed into Dense layer
    Dense(64, activation='relu'), # Hidden layer
    Dense(1, activation='sigmoid') # Output layer for binary classification
])
# Compile the model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
# Train the model
model.fit(
    X_train, y_train,
    epochs=8,
    batch_size=64,
    validation_split=0.2
)
# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Accuracy:", accuracy)


Q3) Deep neural network (Reuters)

import tensorflow as tf
from tensorflow.keras.datasets import reuters
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
import numpy as np
# Load Reuters dataset (top 10,000 most common words)
(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000)
# Function to one-hot encode the input sequences
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1.
  return results
# Vectorize input data
x_train = vectorize_sequences(x_train)
x_test = vectorize_sequences(x_test)
# One-hot encode labels (46 categories)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
# Build a simple deep neural network
model = Sequential([
  Dense(64, activation='relu', input_shape=(10000,)),
  Dense(64, activation='relu'),
  Dense(46, activation='softmax') # 46 output classes
])
# Compile the model
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
# Train the model
model.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2)
# Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)


Q4) CNN (MNIST)

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
# Load MNIST data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Normalize and reshape input
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0
# One-hot encode labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
# Build CNN model
model = Sequential([
  Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1)),
  MaxPooling2D(pool_size=2),
  Conv2D(64, kernel_size=3, activation='relu'),
  MaxPooling2D(pool_size=2),
  Flatten(),
  Dense(128, activation='relu'),
  Dense(10, activation='softmax')
])
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# Train the model
model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)
# Evaluate the model
loss, acc = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {acc:.4f}")



Q5) MNIST (using LSTM pre trained model)

import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
# Load MNIST dataset
X, Y = fetch_openml('mnist_784', version=1, return_X_y=True)
X = X.astype(np.float32) / 255.0 # Normalize pixel values
# Split into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)
# Train MLP classifier
clf = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=20, random_state=42)
clf.fit(X_train, Y_train)
# Predict and evaluate
Y_pred = clf.predict(X_test)
accuracy = accuracy_score(Y_test, Y_pred)
print("Test Accuracy:", accuracy)


Q6) Time Series RNN/LSTM

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
# 1. Generate synthetic sine wave data with noise
def generate_series(n_points=500):
  x = np.arange(n_points)
  return np.sin(0.1 * x) + np.random.normal(scale=0.1, size=n_points)
series = generate_series()
# 2. Normalize the data
scaler = MinMaxScaler()
series_scaled = scaler.fit_transform(series.reshape(-1, 1))
# 3. Create sequences
def create_sequences(data, seq_length):
  X, y = [], []
  for i in range(len(data) - seq_length):
    X.append(data[i:i+seq_length])
    y.append(data[i+seq_length])
  return np.array(X), np.array(y)
seq_length = 20
X, y = create_sequences(series_scaled, seq_length)
# 4. Split into train and test sets
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]
# 5. Build LSTM model
model = Sequential([
  LSTM(64, input_shape=(seq_length, 1)),
  Dense(32, activation='relu'),
  Dense(1)
])
model.compile(optimizer='adam', loss='mse')
# 6. Train the model
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)
# 7. Make predictions
y_pred = model.predict(X_test)
# 8. Inverse transform predictions and true values
y_pred_inverse = scaler.inverse_transform(y_pred)
y_test_inverse = scaler.inverse_transform(y_test)
# 9. Plot results
plt.figure(figsize=(12, 6))
plt.plot(y_test_inverse, label='True')
plt.plot(y_pred_inverse, label='Predicted')
plt.title("LSTM Time-Series Forecasting")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.legend()
plt.show()


Q7) Auto-encoder 

import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# 1. Load and preprocess MNIST dataset
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train[..., tf.newaxis]  # Add channel dimension
x_test = x_test[..., tf.newaxis]

# 2. Define the Autoencoder
input_img = tf.keras.Input(shape=(28, 28, 1))

# Encoder
x = layers.Flatten()(input_img)
x = layers.Dense(128, activation='relu')(x)
encoded = layers.Dense(64, activation='relu')(x)
# Decoder
x = layers.Dense(128, activation='relu')(encoded)
x = layers.Dense(28 * 28, activation='sigmoid')(x)
decoded = layers.Reshape((28, 28, 1))(x)


# Autoencoder Model
autoencoder = tf.keras.Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# 3. Train the Model
autoencoder.fit(x_train, x_train,
                epochs=5,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))

# 4. Visualize Original vs Reconstructed Images
decoded_imgs = autoencoder.predict(x_test[:10])

plt.figure(figsize=(10, 4))
for i in range(10):
    # Original
    plt.subplot(2, 10, i + 1)
    plt.imshow(x_test[i].squeeze(), cmap='gray')
    plt.axis('off')

    # Reconstructed
    plt.subplot(2, 10, i + 11)
    plt.imshow(decoded_imgs[i].squeeze(), cmap='gray')
    plt.axis('off')

plt.tight_layout()
plt.show()

